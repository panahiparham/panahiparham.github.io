<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Parham Panahi's Research Page</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>

    <div class="container">
        <div class="details">
            <h1>Parham Mohammad Panahi</h1>
            <div>
                <p>M.Sc. Student<br>
                <i>Department of Computing Science</i><br>
                <i>University of Alberta</i><br></p>
            </div>
            <div>
                <p>
                    <a href="mailto:parham1@ualberta.ca">parham1@ualberta.ca</a><br>
                    (780) 937-1410
                </p>
            </div>
            <div>
                <p>
                    I am a graduate student of Computing Science at the University of Alberta. I am advised by <a href="http://adamwhite.ca/">Adam White</a>, working in the <a href="http://rlai.ualberta.ca/">RLAI lab</a>.
                    Before that, I graduated with a B.Sc. in Computer Science from Azad University of Tehran in 2021.
                </p>
            </div>
            
            <div class="research-section">
                <h2>Research</h2>
                <!-- <em>Under Construction!!!</em>
                <p>Keywords: Reinforcement Learning</p> -->
                <p>I am interested in creating intelligent agents that organize and (re-)use their experience to learn about and make better decisions in the world.</p>

                <p>One approach to the organization of experience is Model-Based Reinforcement Learning, where the agent creates a model of its environment and imagines possible interactions with it to plan its future. However, regular one-step prediction models are limited in long-horizon planning due to the propagation of errors in multi-step prediction. One way to attack this issue is by incorporating models that consider macro actions, also known as options. Another benefit of these <b>Temporally Abstract Models</b> is the improved performance achieved by <b>speeding up information propagation</b> in the state space.</p>
                
                <p>Most supervised learning systems today train Neural Networks with mini-batches of i.i.d. data. These constraints are against the ideas of sequentiality and incremental updates at the heart of Reinforcement Learning. <b>Experience Replay (ER)</b> reconciles NNs with RL by storing recent interactions of the agent in a buffer and samples mini-batches of data from it to train the agent. ER is a critical part of any Deep RL agent, but it is still not fully understood.</p>
            </div>
            
            <div class="cv-section">
                <h2>Curriculum Vitae</h2>
                <p>My current CV can be found <a href="docs/cv_july2023.pdf" target="_blank">here</a>.</p>
            </div>
        </div>
        <div class="image">
            <img src="img/portrait.JPG" width="250">
        </div>
    </div>
    
</body>
</html>

