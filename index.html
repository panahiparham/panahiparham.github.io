<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Parham Panahi's Research Page</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>

    <div class="container">
        <div class="details">
            <h1>Parham Mohammad Panahi</h1>
            <div>
                <p>M.Sc. Student<br>
                <i>Department of Computing Science</i><br>
                <i>University of Alberta</i><br></p>
            </div>
            <div>
                <p>
                    <a href="mailto:parham1@ualberta.ca">parham1@ualberta.ca</a><br>
                    (780) 937-1410
                </p>
            </div>
            <div>
                <p>
                    I am a graduate student of Computing Science at the University of Alberta. I am advised by <a href="http://adamwhite.ca/">Adam White</a>, working in the <a href="http://rlai.ualberta.ca/">RLAI lab</a>.
                    Before that, I graduated with a B.Sc. in Computer Science from Azad University of Tehran in 2021.
                </p>
            </div>
            
            <div class="research-section">
                <h2>Research</h2>

                <p>I aim to create goal-driven agents that learn in a continually changing environment. To achieve this, agents must use their experience effectively. In the context of <b>Deep Reinforcement Learning</b>, there are broadly two ways to incorporate past data into the learning process: Model learning and experience replay.</p>

                <p>Agents can use past data to construct a model of the world and use it to predict the future, make decisions, and learn from simulated experiences. I worked on a novel approach to construct and use models, <b>Goal Space Planning</b>, which incorporates subtasks and option models to reason about long-horizon events and speed up learning.</p>
                
                <p>An alternative to model learning is a mechanism for storing and reusing experience for direct learning called Experience Replay. Replaying past experiences has become ubiquitous in reinforcement learning with numerous modifications to the originally proposed method. My research aims to improve the understanding of different <b>replay styles</b> in decision-making agents.</p>

                <p>A successful learning agent will continue to learn and explore in a changing world. Most learning systems today lose their ability to learn over time due to a collection of phenomena collectively known as loss of plasticity. I have contributed to the study of loss of plasticity by introducing new <b>environments and evaluation frameworks</b> to lay the groundwork to develop continual learning agents.</p>
            </div>
            
            <div class="cv-section">
                <h2>Curriculum Vitae</h2>
                <p>My current CV can be found <a href="docs/cv.pdf" target="_blank">here</a>.</p>
            </div>
        </div>
        <div class="image">
            <img src="img/portrait.JPG" width="250">
        </div>
    </div>
    
</body>
</html>

