<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Parham Panahi's Research Page</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>

    <div class="container">
        <div class="details">
            <h1>Parham M. Panahi</h1>
            <div>
                <p>M.Sc. Student<br>
                <i>Department of Computing Science</i><br>
                <i>University of Alberta</i><br></p>
            </div>
            <div>
                <p>
                    <a href="mailto:parham1@ualberta.ca">parham1@ualberta.ca</a><br>
                    (780) 937-1410
                </p>
            </div>
            <div>
                <p>
                    I am a graduate student of Computing Science at the University of Alberta. I am advised by <a href="http://adamwhite.ca/">Adam White</a>, working in the <a href="http://rlai.ualberta.ca/">RLAI lab</a>.
                    Before that, I graduated with a B.Sc. in Computer Science from Azad University of Tehran in 2021.
                </p>
            </div>
            
            <div class="research-section">
                <h2>Research</h2>

                <p>I aim to create goal-driven agents that learn in a continually changing environment. To achieve this, agents must use their experience effectively. In the context of <b>Deep Reinforcement Learning</b>, there are broadly two ways to incorporate past data into the learning process: Model learning and experience replay.</p>

                <p>Agents can use past data to construct a model of the world and use it to predict the future, make decisions, and learn from simulated experiences. I worked on a novel approach to construct and use models, <b>Goal Space Planning</b>, which incorporates subtasks and option models to reason about long-horizon events and speed up learning.</p>
                
                <p>An alternative to model learning is a mechanism for storing and reusing experience for direct learning called Experience Replay. Replaying past experiences has become ubiquitous in reinforcement learning with numerous modifications to the originally proposed method. My research aims to improve the understanding of different <b>replay styles</b> in decision-making agents.</p>

                <p>A successful learning agent will continue to learn and explore in a changing world. Most learning systems today lose their ability to learn over time due to a collection of phenomena collectively known as loss of plasticity. I have contributed to the study of loss of plasticity by introducing new <b>environments and evaluation frameworks</b> to lay the groundwork to develop continual learning agents.</p>
            </div>
            
            <div class="cv-section">
                <h2>Curriculum Vitae</h2>
                <p>My current CV can be found <a href="docs/cv.pdf" target="_blank">here</a>.</p>
            </div>
            <div class="publication-section">

                <h2>Conference Papers</h2><br>

                <b>Panahi P. M.</b>, Patterson A., White M., White A. (2024). <a href="#">Investigating the Interplay of Prioritized Replay and Generalization</a>. <i>RLC 2024</i>. <br><br>

                Mesbahi G., <b>Panahi P. M.</b>, Mastikhina O., White M., White A. (2024). <a href="https://arxiv.org/abs/2404.02113">K-percent Evaluation for Lifelong RL</a>. <i>Preprint</i>. <br><br>


                <h2>Journal Papers</h2><br>

                Lo C., Roice K., <b>Panahi P. M.</b>, Jordan S., White A., Mihucz G., Aminmansour F., White M. (2024). <a href="https://arxiv.org/abs/2206.02902">Goal-Space Planning With Subgoal Models</a>. <i>Preprint</i>. <br><br>

                <h2>Workshop Papers</h2><br>

                Roice K., <b>Panahi P. M.</b>, Jordan S., White A., White M. (2024). <a href="http://arxiv.org/abs/2406.01562">A New View on Planning in Online Reinforcement Learning.</a> <i>PRL workshop ICAPS 2024</i>. <br><br>

            </div>
            <div class="scholar-section">
                <h2>Google Scholar</h2>
                <p>My Google Scholar page can be found <a href="https://scholar.google.ca/citations?user=LCRB4rcAAAAJ&hl=en" target="_blank">here</a>.</p>
            </div>
        </div>
        <div class="image">
            <!-- <img src="img/portrait.JPG" width="250"> -->
            <img src="img/parham-may24.jpeg" width="300">
        </div>
    </div>
    
</body>
</html>

