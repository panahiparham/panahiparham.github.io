<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Parham Panahi's Research Page</title>

    <meta name="author" content="Parham Mohammad Panahi">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Parham Mohammad Panahi
                </p>
                <p>I am a machine learning and reinforcement learning researcher at the University of Alberta, working with <a href="http://adamwhite.ca/" target="_blank">Adam White</a>. I am also affiliated with the <a href="http://rlai.ualberta.ca/" target="_blank">RLAI lab</a> and the Alberta Machine Intelligence Institute (<a href="https://www.amii.ca" target="_blank">amii</a>). Before that, I graduated with a M.Sc. in Computing Science from the University of Alberta in 2024.
                </p>
                <p style="text-align:center">
                  <a href="mailto:parham1@ualberta.ca">Email</a> &nbsp;/&nbsp;
                  <a href="data/ParhamPanahi-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.ca/citations?user=LCRB4rcAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/panahiparham">Github</a> &nbsp;/&nbsp;
                  <a href="https://era.library.ualberta.ca/items/adcce5fe-6a7b-4f95-8a36-8db0b0731877">M.Sc. Thesis</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/parham-may24.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/parham-may24.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                <br/>Keywords: <i>Reinforcement Learning, Representation Learning, Continual Learning, Model-Based RL, Optimization, and Machine Learning.</i><br/>
                <p>I am broadly interested in machine learning and reinforcement learning. My research goal is to create efficient and reliable learning systems capable of continual interaction and decision-making in a changing world.</p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr>
      <td style="padding:8px;width:30%;vertical-align:middle">
        <div>
          <img src='images/panahi2024investigating.svg' width=100%>
        </div>
      </td>
      <td style="padding:8px;width:70%;vertical-align:middle">
        <a href="https://rlj.cs.umass.edu/2024/papers/Paper265.html">
			<span class="papertitle">Investigating the Interplay of Prioritized Replay and Generalization</span>
        </a>
        <br>
				<strong>Parham Mohammad Panahi</strong>,
        Andrew Patterson,
        Martha White,
        Adam White
				<br>
        <em>Reinforcement Learning Conference (RLC)</em>, 2024
        <br>
        <p>
        We present insight into the interaction between prioritization, bootstrapping, and neural networks and propose several improvements for prioritized replay in tabular settings and noisy domains.
        </p>
      </td>
    </tr>
    <tr>
      <td style="padding:8px;width:30%;vertical-align:middle">
        <div>
          <img src='images/lo2024goal.png' width=100%>
        </div>
      </td>
      <td style="padding:8px;width:70%;vertical-align:middle">
        <a href="https://www.jmlr.org/papers/v25/24-0040.html">
			<span class="papertitle">Goal-Space Planning with Subgoal Models</span>
        </a>
        <br>
        *Chunlok Lo,
        *Kevin Roice,
        <strong>*Parham Mohammad Panahi</strong>,
        Scott M. Jordan,
        Adam White,
        Gabor Mihucz,
        Farzane Aminmansour,
        Martha White
				<br>
        <em>Journal of Machine Learning Research (JMLR)</em>, 2024
        <br>
        <p>
        We constrain background planning to a given set of (abstract) subgoals and learning only local, subgoal-conditioned models to avoid compounding model error. Also <a href="https://arxiv.org/abs/2406.01562">appeared</a> as an oral presentation at Planning and Reinforcement Learning Workshop at ICAPS 2024.</p>
        </p>
        <em>* indicates equal contribution</em>
      </td>
    </tr>

    <tr>
      <td style="padding:8px;width:30%;vertical-align:middle">
        <div>
          <img src='images/mesbahi2024kpercent.png' width=100%>
        </div>
      </td>
      <td style="padding:8px;width:70%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2404.02113">
			<span class="papertitle">K-percent Evaluation for Lifelong RL</span>
        </a>
        <br>
        Golnaz Mesbahi,
        <strong>Parham Mohammad Panahi</strong>,
        Olya Mastikhina,
        Martha White,
        Adam White
				<br>
        <em>Preprint</em>, 2024
        <br>
        <p>
        We propose a new approach for evaluating lifelong RL agents where only k percent of the experiment data can be used for hyperparameter tuning. We find algorithms deisgned to maintain network plasticity perform well under this evaluation scheme.
        </p>
      </td>
    </tr>

    </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Recorded Talks</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr>
      <td style="padding:8px;width:30%;vertical-align:middle">
        <div>
          <img src='images/ttt2024.jpg' width=100%>
        </div>
      </td>
      <td style="padding:8px;width:70%;vertical-align:middle">
        <a href="https://www.youtube.com/watch?v=Ac9Jm6FDFGs">
			<span class="papertitle">Experience Selection in Deep RL</span>
        </a>
        <br>
        <p>Tea Time Talks 2024, University of Alberta, Alberta Machine Intelligence Institute (amii).</p>
      </td>
    </tr>
    </tbody></table>

  </body>
</html>
